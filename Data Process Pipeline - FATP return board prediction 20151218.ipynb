{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The purpose of this data process pipeline is to generate a DataFrame for FATP return board machine learning modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess steps\n",
    "Step 1. Download necessary data (using HIVE) and tranform to DataFrame <br />\n",
    "    - FCT from test log csv (1 week)\n",
    "    - GateKeeper from Bobcat (2 weeks)\n",
    "    - RPC (4 weeks)\n",
    "Step 2. Join and filter data (X) <br />\n",
    "Step 3. Filter RPC data (y) <br />\n",
    "Step 4. Extract FCT test values (FCT['items']) and store it as a separate DataFrame <br />\n",
    "Step 5. Missing value handling before data scalling <br />\n",
    "Step 6. Data Scalling (Normalization, Max-Min Scalling) <br />\n",
    "Step 7. Missing value handling after data scalling <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Download necessary data and tranform to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. ssh log in server **(10.195.223.53)** and download test log data with user specified date and period with HIVE, e.g station = **FCT**, date = 2015-10-26, period = 6 days (a week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh mlb@10.195.223.53 \"hive -e \\\"use cpk; select * from mlb_test_log_detail \\\n",
    "                        where station = 'FCT'\\\n",
    "                        and model = 'N71'\\\n",
    "                        and hour between '2015-02-19_00' and '2015-10-31_23';\\\"\" \\\n",
    "                        > Data/FCT_20151026.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Download **BOBCAT** data with same date and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ssh mlb@10.195.223.53 \"hive -e \\\"use cpk; select * from mlb_bobcat_raw \\\n",
    "                        where model = 'Agera'\\\n",
    "                        and day between '2015-10-26' and '2015-11-07';\\\"\" \\\n",
    "                        > Data/Bobcat_20151026.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Download RPC data with same starting date but period = 4 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ssh mlb@zz2 \"hive -e \\\"use cpk; select * from rpc_file\\\n",
    "                        where day between '2015-10-26' and '2015-11-21';\\\"\" \\\n",
    "                        > Data/rpc_20151026.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Transform downloaded data to DataFrame\n",
    "\n",
    "In the following steps, we will use SparkSQL DataFrame to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import findspark\n",
    "findspark.init('/Users/hadoop1/srv/spark')\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, HiveContext, Row\n",
    "import pandas as pd\n",
    "sc = pyspark.SparkContext()\n",
    "hc = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctLines = sc.textFile(\"Data/FCT_20151026.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fctParts = fctLines.map(lambda l: l.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fctBoard = fctParts.map(lambda p: Row(serial_number=p[0], test_result=p[1], fct_test_time=p[2],\\\n",
    "                                     version=p[3],line=p[4],machine=p[5],\\\n",
    "                                     slot=p[7],items=p[8],model=p[10],station=p[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctBoardDf = hc.createDataFrame(fctBoard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Bobcat **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0\twip_no\tstring\t\n",
    "1\ttest_time\tstring\t\n",
    "2\ttest_hour\tstring\t\n",
    "3\tis_test_fail\tstring\t\n",
    "4\tsymptom_code\tstring\t\n",
    "5\tsymptom_code_first\tstring\t\n",
    "6\tfactory\tstring\t\n",
    "7\tstation\tstring\t\n",
    "8\tstation_code\tstring\t\n",
    "9\tline\tstring\t\n",
    "10\tmachine\tstring\t\n",
    "11\tline_type\tstring\t\n",
    "12\ttest_times\tint\t3\n",
    "13\trankno\tint\t\n",
    "14\tfail_count\tint\t\n",
    "15\ttest_result\tstring\t\n",
    "16\tsymptom\tstring\t\n",
    "17\tday\tstring\t\n",
    "18\tmodel\tstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bobcatLines = sc.textFile('Data/Bobcat_20151026.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bobcatParts = bobcatLines.map(lambda l: l.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bobcatRows = bobcatParts.map(lambda p: Row(sympton = p[4], serial_num = p[0],test_time=p[1],station=p[7],test_result=p[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bobcatDf = hc.createDataFrame(bobcatRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. RPC Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \tName\tType\tComment\n",
    "0\tnamec\tstring\t\n",
    "1\tstation_code\tstring\t\n",
    "2\tserial_num\tstring\t\n",
    "3\tadd_date\tstring\t\n",
    "4\temp\tstring\t\n",
    "5\tstation_type\tstring\t\n",
    "6\tfail_location\tstring\t\n",
    "7\tcode\tstring\t\n",
    "8\tdesce\tstring\t\n",
    "9\tother\tstring\t\n",
    "10\tday\tstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rpcLines = sc.textFile('Data/rpc_20151026.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rpcParts = rpcLines.map(lambda l: l.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpcRows = rpcParts.map(lambda p: Row(namec = p[0],serial_num=p[2], add_date = p[3], emp = p[4],\\\n",
    "                                     fail_location=p[6], code = p[7], desce = p[8], day = p[10],\\\n",
    "                                     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpcDf = hc.createDataFrame(rpcRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3. Join and filter data (X)\n",
    "Now we have 3 dataframes, fctBoardDf, bobcatDf and rpcDf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctBoardDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bobcatDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpcDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And 3 temp tables for sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fctBoardDf.registerTempTable(\"fctBoardDfTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bobcatDf.registerTempTable(\"bobcatDfTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rpcDf.registerTempTable(\"rpcDfTemp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Join FCT ['test_result']==pass and Bobcat GATEKEEPER ['test_result']==First Pass I ['test_result']==Retest Pass  DataFrames on serial_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First filter fctBoardDf DF with only PASS results, and verify its numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fctBoardPassDf = fctBoardDf.filter(fctBoardDf.test_result == 'PASS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fctBoardFailDf = fctBoardDf.filter(fctBoardDf.test_result == 'FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use bobcatGkDf and filter out serial numbers that passed 'GATEKEEPER-PREBURN' stations. This filtered DF then work as a mask for fctBoardDfPass to make sure all serial numbers are passed at the last station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bobcatGkPassDf = bobcatDf.filter((bobcatDf.station == 'GATEKEEPER-PREBURN'))\\\n",
    "                         .filter((bobcatDf.test_result == 'First Pass') | \n",
    "                                   (bobcatDf.test_result == 'Retest Pass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctBoardPassDf.registerTempTable(\"fctBoardPassDfTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bobcatGkPassDf.registerTempTable(\"bobcatGKPassDfTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctGateKeeper = fctBoardPassDf.join(bobcatGkPassDf, fctBoardPassDf.serial_number == bobcatGkPassDf.serial_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fctGateKeeper, fctGateKeeperSql are DFs that contain records that have passed FCT and GateKeeper test stations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Join fctGateKeeperSql DF with bobcatDfFctPass DF on [serial_num] and [test_start_time] columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First join fctGateKeeper DF with bobcatDf DF to infer \"First Pass\" and \"Retest Pass\" information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bobcatFctFirstPassDf = bobcatDf.filter(bobcatDf.station == 'FCT')\\\n",
    "                           .filter(bobcatDf.test_result == 'First Pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fctGateKeeper.registerTempTable(\"fctGateKeeperTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bobcatFctFirstPassDf.registerTempTable(\"bobcatFctFirstPassDfTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctGateKeeperFirstPassDf = hc.sql(\"select F.serial_number, F.line, F.machine,\\\n",
    "                                      F.model, F.slot, F.fct_test_time, F.items, B.test_result\\\n",
    "                                      from fctGateKeeperTemp F\\\n",
    "                                      inner join bobcatFctFirstPassDfTemp B\\\n",
    "                                      on F.serial_number =\\\n",
    "                                      B.serial_num and F.fct_test_time = B.test_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fctGateKeeperFirstPassDf.registerTempTable(\"fctGateKeeperFirstPassDfTemp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a new column specify item numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some records contain less than 799 columnn, the reason is still unknown. In order to remove thos incorrect records, we creat a column that specify the number of test items of the record and exclude records according to item number criteria, e.g, item number > 799. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions should be import from pyspark.sql, then we can use functions.udf to defince User Defined Functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining user defiend funciton, the type of the returned value should be specified beforehand, therefore the IntegerType shold also be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def itemToNum(items):\n",
    "    dic = ast.literal_eval(items)\n",
    "    return len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkItemToNum = functions.udf(lambda items: len(ast.literal_eval(items)), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctGateKeeperFirstPassItemnumDf = fctGateKeeperFirstPassDf.withColumn('item_num',\\\n",
    "                                  sparkItemToNum(fctGateKeeperFirstPassDf.items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctGateKeeperFirstPass799Df = fctGateKeeperFirstPassItemnumDf.filter\\\n",
    "                        (fctGateKeeperFirstPassItemnumDf.item_num == 799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctGateKeeperFirstPass799Df.registerTempTable(\"fctGateKeeperFirstPass799DfTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctGatekeeperAllpass799 = hc.sql(\"select * from fctGateKeeperFirstPass799DfTemp F\\\n",
    "                               left outer join rpcDfTemp R\\\n",
    "                               on F.serial_number = R.serial_num\\\n",
    "                               where R.serial_num is null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fctGatekeeperCRBFail799 = hc.sql(\"select * from fctGateKeeperFirstPass799DfTemp F\\\n",
    "                            left semi join rpcDfTemp R\\\n",
    "                            on F.serial_number = R.serial_num\\\n",
    "                            and R.namec = 'CRB Check In'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Filter RPC data (y)  ------ Pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Filter RPC DataFrame by ['namec'] column and separate ['namec']=='TFB Check In' and ['namec']=='CRB Check In'. We need only 'CRB Check In' records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Filter RPC records by 'NTF' and 'Replaced'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Filter 'Replaced' records by 'AP' and 'RF'\n",
    "\n",
    "Note: AP-Application, RF-Radio Frequency\n",
    "      FCT test items are mainly for AP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Filter RPC DataFrame by ['serial_num'].isin(above DataFrame['serial_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Join above DataFrame with RPC DataFrame to identify FATP return records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Extract FCT test values (FCT['items']) and store it as a separate DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. The test values of FCT test station are stored as mapped file within FCT['items'] column. Extract these values and stored them as a separate DataFrame. Rename columns of original DataFrame and stored as a metadata DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, we have joined and generated 4 dataframes:**\n",
    "- fctGatekeeperAllpass(223041)\n",
    "- fctGatekeeperCRBFail(3126)\n",
    "\n",
    "- fctGatekeeperAllpass799(222836)\n",
    "- fctGatekeeperCRBFail799(3117)\n",
    "\n",
    "Both dataframes have 1 column called 'items' that contain FCT test log values, that will be used for building anomaly detection model and for 異常群集辨識. We will first extract FCT test log values from this 2 dataframes and stored them as a separate dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dic_to_row(record):\n",
    "    schema = {'{i:s}'.format(i = key):record[key] for key in record}\n",
    "    return Row(**schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsCRBFailRow = fctGatekeeperCRBFail799.map(lambda row: row.items)\\\n",
    "                                .map(lambda s: ast.literal_eval(s))\\\n",
    "                                .map(lambda d: dic_to_row(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsCRBFailDf = hc.createDataFrame(itemsCRBFailRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsCRBFailDf = itemsCRBFailDf.withColumn('fatp_return', F.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsCRBFailDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemsCRBFailValues = itemsCRBFailDf.map(lambda r: r.asDict())\\\n",
    "                                   .map(lambda d: d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsCRBFailValues.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsCRBFailValues = itemsCRBFailDf.map(lambda r: r.asDict())\\\n",
    "                                   .map(lambda d: d.values())\\\n",
    "                                   .map(toCSVLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = itemsCRBFailValues.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsAllpassRow = fctGatekeeperAllpass799.map(lambda row: row.items)\\\n",
    "                                      .map(lambda items: ast.literal_eval(items))\\\n",
    "                                      .map(lambda dic: dic_to_row(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsAllpassDf = hc.createDataFrame(itemsAllpassRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemsAllpassDf = itemsAllpassDf.withColumn('fatp_return', F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemsAllpassValues = itemsAllpassDf.map(lambda r: r.asDict())\\\n",
    "                                   .map(lambda d: d.values())\\\n",
    "                                   .map(toCSVLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allPass = itemsAllpassValues.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsAllpassDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsAllpassPdf = itemsAllpassDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** At this point, we have separate test log as a different dataframe, therefore now we have 4 dataframes:**\n",
    "\n",
    "(without excluding S/N having less than 799 items) \n",
    "1. fctGatekeeperAllpass (223041)\n",
    "2. itemsAllpassDf (223041) \n",
    "3. fctGatekeeperCRBFail (3126)\n",
    "4. itemsCRBFailDf (3126)\n",
    "\n",
    "(after excluding S/N having less than 799 items)\n",
    "1. fctGatekeeperAllpass799 (222836)\n",
    "2. itemsAllpassDf (222836)\n",
    "3. fctGatekeeperCRBFail799 (3117)\n",
    "4. itemsCRBFailDf (3117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Missing value handling before data scalling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Examine missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Data Scalling (Normalization, Max-Min Scalling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Rescale FCT test log DataFrame with normalization and max-min scalling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Missing value handling after data scalling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### a. Replace missing value with column.min()\n",
    "\n",
    "Note: Most of the observed missing value were due to incorrect scientific notation. Whenver the number is too small, the scientific notation will be displayed incorrectly, therefore we take min() value of each column to fill out missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
